{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collection of Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legitimate_urls = pd.read_csv(\"legitimate-urls.csv\")\n",
    "phishing_urls = pd.read_csv(\"phishing-urls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legitimate_urls.head(10)\n",
    "phishing_urls.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data PreProcessing\n",
    "#### Data is in two data frames so we merge them to make one dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = legitimate_urls.append(phishing_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = urls.drop(urls.columns[[0,3,5]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling the rows in the dataset so that when splitting the train and test set are equally distributed\n",
    "urls = urls.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing class variable from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_without_labels = urls.drop('label',axis=1)\n",
    "urls_without_labels.columns\n",
    "labels = urls['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### splitting the data into train data and test data\n",
    "\n",
    "Dividing the data in the ratio of 70:30 [train_data:test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(urls_without_labels, labels, test_size=0.30, random_state=110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_train),len(data_test),len(labels_train),len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### checking the split of labels in train and test data\n",
    "\n",
    "The split should be in equal proportion for both classes\n",
    "\n",
    "Phishing - 1\n",
    "\n",
    "Legitimate - 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initially checking the split of labels_train data \n",
    "labels_train.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the split for labels_test data\n",
    "labels_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as the split is almost in equal proportion we can train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the model and fitting the data into the model\n",
    "\n",
    "creating the model with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier.fit(data_train,labels_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the result for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_label = random_forest_classifier.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating confusion matrix and checking the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "cpnfusionMatrix = confusion_matrix(labels_test,prediction_label)\n",
    "print(cpnfusionMatrix)\n",
    "accuracy_score(labels_test,prediction_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the efficiency of model by specifying max_depth as well as number of tress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_random_forest_classifier = RandomForestClassifier(n_estimators=500, max_depth=20, max_leaf_nodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_random_forest_classifier.fit(data_train,labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_classifier_prediction_label = custom_random_forest_classifier.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "confusionMatrix2 = confusion_matrix(labels_test,custom_classifier_prediction_label)\n",
    "print(confusionMatrix2)\n",
    "accuracy_score(labels_test,custom_classifier_prediction_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#feature_importances_ : array of shape = [n_features] ------ The feature importances (the higher, the more important the feature).\n",
    "\n",
    "#feature_importances_  -- This method returns the quantified relative importance in the order the features were fed to the algorithm\n",
    "\n",
    "importances = custom_random_forest_classifier.feature_importances_\n",
    "\n",
    "#std = np.std([tree.feature_importances_ for tree in custom_random_forest_classifier.estimators_],axis=0)   #[[[estimators_ :explaination ---  list of DecisionTreeClassifier ----- (The collection of fitted sub-estimators.)]]]\n",
    "\n",
    "#To make the plot pretty, weâ€™ll instead sort the features from most to least important.\n",
    "indices = np.argsort(importances)[::-1] \n",
    "print(f\"indices of columns : {indices}\")\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"\\n ***Feature ranking: *** \\n\")\n",
    "print(\"Feature name : Importance\")\n",
    "\n",
    "for f in range(data_train.shape[1]):\n",
    "    print(f\"{f+1} {data_train.columns[indices[f]]}   :  {importances[indices[f]]} \\n\")\n",
    "    \n",
    "print(\"**** The blue bars are the feature importances of the randomforest classifier, along with their inter-trees variability*****\")\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(data_train.shape[1]), importances[indices],\n",
    "       color=\"b\", align=\"center\")   \n",
    "#yerr=std[indices] -- this is another parameter that can be included if std is calculated above\n",
    "#and also it gives error bar that's the reason we calculate std above. but here we are not making it plot.\n",
    "\n",
    "plt.xticks(range(data_train.shape[1]), data_train.columns[indices])\n",
    "plt.xlim([-1, data_train.shape[1]])\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (35,15)  #this will increase the size of the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#click on the image to get clear view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
